{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "#sentiment analysis\n",
    "from textblob import TextBlob\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#keyword Extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "#visuals\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load bank_reviews dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>bank</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the app is proactive and a good connections.</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-06-05</td>\n",
       "      <td>CBE</td>\n",
       "      <td>Google Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I cannot send to cbebirr app. through this app.</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-06-05</td>\n",
       "      <td>CBE</td>\n",
       "      <td>Google Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>good</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-06-05</td>\n",
       "      <td>CBE</td>\n",
       "      <td>Google Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>not functional</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-06-05</td>\n",
       "      <td>CBE</td>\n",
       "      <td>Google Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>everytime you uninstall the app you have to re...</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-06-04</td>\n",
       "      <td>CBE</td>\n",
       "      <td>Google Play</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  rating        date bank  \\\n",
       "0       the app is proactive and a good connections.       5  2025-06-05  CBE   \n",
       "1    I cannot send to cbebirr app. through this app.       3  2025-06-05  CBE   \n",
       "2                                               good       4  2025-06-05  CBE   \n",
       "3                                     not functional       1  2025-06-05  CBE   \n",
       "4  everytime you uninstall the app you have to re...       1  2025-06-04  CBE   \n",
       "\n",
       "        source  \n",
       "0  Google Play  \n",
       "1  Google Play  \n",
       "2  Google Play  \n",
       "3  Google Play  \n",
       "4  Google Play  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('bank_reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6670 entries, 0 to 6669\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   review  6669 non-null   object\n",
      " 1   rating  6670 non-null   int64 \n",
      " 2   date    6670 non-null   object\n",
      " 3   bank    6670 non-null   object\n",
      " 4   source  6670 non-null   object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 260.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            the app is proactive and a good connections.\n",
       "1         I cannot send to cbebirr app. through this app.\n",
       "2                                                    good\n",
       "3                                          not functional\n",
       "4       everytime you uninstall the app you have to re...\n",
       "                              ...                        \n",
       "6665                                            Nice one.\n",
       "6666                           ·â†·å£·àù ·ã∞·àµ ·ã®·àö·àç ·àà·ãç·å• ·ä†·äì·àò·à∞·åç·äì·àà·äï üá™üáπ\n",
       "6667         Best applicationüëçThank you ! Abyssinia bank.\n",
       "6668    Absolutely it's fantastic apps this New apps i...\n",
       "6669           The best app next to Tele birr in ethiopia\n",
       "Name: review, Length: 6670, dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Data¬∂\n",
    "Steps:\n",
    "\n",
    "1. Clean text: Remove punctuation, special characters, convert to lowercase.\n",
    "2. Tokenize: Split text into words.\n",
    "3. Remove stop words: Eliminate common words (e.g., \"the,\" \"and\").\n",
    "4. Lemmatize: Reduce words to base form (e.g., \"running\" ‚Üí \"run\").\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As shown the reveiw column in our data contains Amharic sentences so I have to translate it first to English\n",
    "### Use Google Translate API\n",
    "### Step 1: Install googletrans\n",
    "### pip install googletrans==4.0.0-rc1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Translate Function\n",
    "### Step 3: Apply Translation to Amharic Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\smith\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\smith\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\smith\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\smith\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Load sentiment pipeline (DistilBERT SST-2)\u001b[39;00m\n\u001b[32m     11\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mdistilbert-base-uncased-finetuned-sst-2-english\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m sentiment_pipeline = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msentiment-analysis\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Load your dataframe (replace with your actual data)\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# df = pd.read_csv('your_reviews.csv')\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Example:\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# df = pd.DataFrame({'review': [\"·ä•·äì·â¥ ·ä†·àõ·à≠·äõ ·äê·ãç\", \"This is great!\", \"·ä†·ã≠ ·ä†·ã≠\", \"I love it!\"]})\u001b[39;00m\n\u001b[32m     18\u001b[39m \n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Language detection function\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdetect_lang\u001b[39m(text):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\KAIM\\Week0_12\\Week_2\\AIM_Week2_Customer_Experience_Analytics\\.venv\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:942\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    941\u001b[39m     model_classes = {\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m: targeted_task[\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m: targeted_task[\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m]}\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     framework, model = \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    952\u001b[39m model_config = model.config\n\u001b[32m    953\u001b[39m hub_kwargs[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m] = model.config._commit_hash\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\KAIM\\Week0_12\\Week_2\\AIM_Week2_Customer_Experience_Analytics\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:243\u001b[39m, in \u001b[36minfer_framework_load_model\u001b[39m\u001b[34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[39m\n\u001b[32m    217\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    218\u001b[39m \u001b[33;03mSelect framework (TensorFlow or PyTorch) to use from the `model` passed. Returns a tuple (framework, model).\u001b[39;00m\n\u001b[32m    219\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    240\u001b[39m \u001b[33;03m    `Tuple`: A tuple framework, model.\u001b[39;00m\n\u001b[32m    241\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available():\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    244\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAt least one of TensorFlow 2.0 or PyTorch should be installed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    245\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTo install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    246\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTo install PyTorch, read the instructions at https://pytorch.org/.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    247\u001b[39m     )\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    249\u001b[39m     model_kwargs[\u001b[33m\"\u001b[39m\u001b[33m_from_pipeline\u001b[39m\u001b[33m\"\u001b[39m] = task\n",
      "\u001b[31mRuntimeError\u001b[39m: At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/."
     ]
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "from transformers import pipeline\n",
    "from deep_translator import GoogleTranslator\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "\n",
    "# Load sentiment pipeline (DistilBERT SST-2)\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model_name)\n",
    "\n",
    "# Language detection function\n",
    "def detect_lang(text):\n",
    "    try:\n",
    "        return detect(str(text)).lower()\n",
    "    except:\n",
    "        return 'unknown'\n",
    "\n",
    "# Translate Amharic to English\n",
    "def translate_amharic_to_english(text):\n",
    "    try:\n",
    "        if not isinstance(text, str) or text.strip() == \"\":\n",
    "            return text\n",
    "        return GoogleTranslator(source='am', target='en').translate(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Translation error: {e}\")\n",
    "        return text  # fallback to original text if translation fails\n",
    "\n",
    "# Text preprocessing: tokenize, lowercase, remove stopwords, lemmatize\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Detect language column\n",
    "df['lang'] = df['review'].apply(detect_lang)\n",
    "\n",
    "# Translate Amharic reviews\n",
    "df['translated_review'] = df.apply(\n",
    "    lambda row: translate_amharic_to_english(row['review']) if row['lang'] == 'am' else row['review'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Preprocess all translated/English reviews\n",
    "df['processed_review'] = df['translated_review'].apply(preprocess_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"  # or np.nan if you want to preserve NaNs\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "# Apply preprocessing\n",
    "df['processed_review'] = df['translated_review'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>bank</th>\n",
       "      <th>source</th>\n",
       "      <th>lang</th>\n",
       "      <th>translated_review</th>\n",
       "      <th>processed_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the app is proactive and a good connections.</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-06-05</td>\n",
       "      <td>CBE</td>\n",
       "      <td>Google Play</td>\n",
       "      <td>en</td>\n",
       "      <td>the app is proactive and a good connections.</td>\n",
       "      <td>app proactive good connection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I cannot send to cbebirr app. through this app.</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-06-05</td>\n",
       "      <td>CBE</td>\n",
       "      <td>Google Play</td>\n",
       "      <td>en</td>\n",
       "      <td>I cannot send to cbebirr app. through this app.</td>\n",
       "      <td>send cbebirr app app</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>good</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-06-05</td>\n",
       "      <td>CBE</td>\n",
       "      <td>Google Play</td>\n",
       "      <td>so</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>not functional</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-06-05</td>\n",
       "      <td>CBE</td>\n",
       "      <td>Google Play</td>\n",
       "      <td>en</td>\n",
       "      <td>not functional</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>everytime you uninstall the app you have to re...</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-06-04</td>\n",
       "      <td>CBE</td>\n",
       "      <td>Google Play</td>\n",
       "      <td>en</td>\n",
       "      <td>everytime you uninstall the app you have to re...</td>\n",
       "      <td>everytime uninstall app reach physically oldy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6665</th>\n",
       "      <td>Nice one.</td>\n",
       "      <td>5</td>\n",
       "      <td>2024-01-14</td>\n",
       "      <td>BOA</td>\n",
       "      <td>Google Play</td>\n",
       "      <td>en</td>\n",
       "      <td>Nice one.</td>\n",
       "      <td>nice one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6666</th>\n",
       "      <td>·â†·å£·àù ·ã∞·àµ ·ã®·àö·àç ·àà·ãç·å• ·ä†·äì·àò·à∞·åç·äì·àà·äï üá™üáπ</td>\n",
       "      <td>5</td>\n",
       "      <td>2024-01-14</td>\n",
       "      <td>BOA</td>\n",
       "      <td>Google Play</td>\n",
       "      <td>unknown</td>\n",
       "      <td>·â†·å£·àù ·ã∞·àµ ·ã®·àö·àç ·àà·ãç·å• ·ä†·äì·àò·à∞·åç·äì·àà·äï üá™üáπ</td>\n",
       "      <td>·â†·å£·àù ·ã∞·àµ ·ã®·àö·àç ·àà·ãç·å• ·ä†·äì·àò·à∞·åç·äì·àà·äï</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6667</th>\n",
       "      <td>Best applicationüëçThank you ! Abyssinia bank.</td>\n",
       "      <td>5</td>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>BOA</td>\n",
       "      <td>Google Play</td>\n",
       "      <td>en</td>\n",
       "      <td>Best applicationüëçThank you ! Abyssinia bank.</td>\n",
       "      <td>best abyssinia bank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6668</th>\n",
       "      <td>Absolutely it's fantastic apps this New apps i...</td>\n",
       "      <td>5</td>\n",
       "      <td>2024-01-11</td>\n",
       "      <td>BOA</td>\n",
       "      <td>Google Play</td>\n",
       "      <td>en</td>\n",
       "      <td>Absolutely it's fantastic apps this New apps i...</td>\n",
       "      <td>absolutely fantastic apps new apps fast good apps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6669</th>\n",
       "      <td>The best app next to Tele birr in ethiopia</td>\n",
       "      <td>5</td>\n",
       "      <td>2024-01-10</td>\n",
       "      <td>BOA</td>\n",
       "      <td>Google Play</td>\n",
       "      <td>en</td>\n",
       "      <td>The best app next to Tele birr in ethiopia</td>\n",
       "      <td>best app next tele birr ethiopia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6670 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review  rating        date  \\\n",
       "0          the app is proactive and a good connections.       5  2025-06-05   \n",
       "1       I cannot send to cbebirr app. through this app.       3  2025-06-05   \n",
       "2                                                  good       4  2025-06-05   \n",
       "3                                        not functional       1  2025-06-05   \n",
       "4     everytime you uninstall the app you have to re...       1  2025-06-04   \n",
       "...                                                 ...     ...         ...   \n",
       "6665                                          Nice one.       5  2024-01-14   \n",
       "6666                         ·â†·å£·àù ·ã∞·àµ ·ã®·àö·àç ·àà·ãç·å• ·ä†·äì·àò·à∞·åç·äì·àà·äï üá™üáπ       5  2024-01-14   \n",
       "6667       Best applicationüëçThank you ! Abyssinia bank.       5  2024-01-12   \n",
       "6668  Absolutely it's fantastic apps this New apps i...       5  2024-01-11   \n",
       "6669         The best app next to Tele birr in ethiopia       5  2024-01-10   \n",
       "\n",
       "     bank       source     lang  \\\n",
       "0     CBE  Google Play       en   \n",
       "1     CBE  Google Play       en   \n",
       "2     CBE  Google Play       so   \n",
       "3     CBE  Google Play       en   \n",
       "4     CBE  Google Play       en   \n",
       "...   ...          ...      ...   \n",
       "6665  BOA  Google Play       en   \n",
       "6666  BOA  Google Play  unknown   \n",
       "6667  BOA  Google Play       en   \n",
       "6668  BOA  Google Play       en   \n",
       "6669  BOA  Google Play       en   \n",
       "\n",
       "                                      translated_review  \\\n",
       "0          the app is proactive and a good connections.   \n",
       "1       I cannot send to cbebirr app. through this app.   \n",
       "2                                                  good   \n",
       "3                                        not functional   \n",
       "4     everytime you uninstall the app you have to re...   \n",
       "...                                                 ...   \n",
       "6665                                          Nice one.   \n",
       "6666                         ·â†·å£·àù ·ã∞·àµ ·ã®·àö·àç ·àà·ãç·å• ·ä†·äì·àò·à∞·åç·äì·àà·äï üá™üáπ   \n",
       "6667       Best applicationüëçThank you ! Abyssinia bank.   \n",
       "6668  Absolutely it's fantastic apps this New apps i...   \n",
       "6669         The best app next to Tele birr in ethiopia   \n",
       "\n",
       "                                       processed_review  \n",
       "0                         app proactive good connection  \n",
       "1                                  send cbebirr app app  \n",
       "2                                                  good  \n",
       "3                                            functional  \n",
       "4     everytime uninstall app reach physically oldy ...  \n",
       "...                                                 ...  \n",
       "6665                                           nice one  \n",
       "6666                            ·â†·å£·àù ·ã∞·àµ ·ã®·àö·àç ·àà·ãç·å• ·ä†·äì·àò·à∞·åç·äì·àà·äï  \n",
       "6667                                best abyssinia bank  \n",
       "6668  absolutely fantastic apps new apps fast good apps  \n",
       "6669                   best app next tele birr ethiopia  \n",
       "\n",
       "[6670 rows x 8 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m nlp = \u001b[43mspacy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43men_core_web_sm\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclean_text\u001b[39m(text):\n\u001b[32m      4\u001b[39m     text = re.sub(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m[^A-Za-z\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms]\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, text.lower())\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\KAIM\\Week0_12\\Week_2\\AIM_Week2_Customer_Experience_Analytics\\.venv\\Lib\\site-packages\\spacy\\__init__.py:52\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m     29\u001b[39m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[32m     30\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] = util.SimpleFrozenDict(),\n\u001b[32m     36\u001b[39m ) -> Language:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[32m     38\u001b[39m \n\u001b[32m     39\u001b[39m \u001b[33;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m \u001b[33;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\KAIM\\Week0_12\\Week_2\\AIM_Week2_Customer_Experience_Analytics\\.venv\\Lib\\site-packages\\spacy\\util.py:484\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m    482\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[32m    483\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors.E941.format(name=name, full=OLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors.E050.format(name=name))\n",
      "\u001b[31mOSError\u001b[39m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text.lower())\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc if not token.is_stop and token.is_alpha])\n",
    "\n",
    "df = pd.read_csv('bank_reviews.csv')\n",
    "df['cleaned_text'] = df['review'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis (VADER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SentimentIntensityAnalyzer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m sia = \u001b[43mSentimentIntensityAnalyzer\u001b[49m()\n\u001b[32m      2\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33msentiment_score\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mreview\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: sia.polarity_scores(x)[\u001b[33m'\u001b[39m\u001b[33mcompound\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      3\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33msentiment_label\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33msentiment_score\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[33m'\u001b[39m\u001b[33mpositive\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x > \u001b[32m0.05\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mnegative\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x < -\u001b[32m0.05\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mneutral\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'SentimentIntensityAnalyzer' is not defined"
     ]
    }
   ],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "df['sentiment_score'] = df['review'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "df['sentiment_label'] = df['sentiment_score'].apply(lambda x: 'positive' if x > 0.05 else 'negative' if x < -0.05 else 'neutral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Sentiment by Bank and Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_summary = df.groupby(['bank_name', 'rating'])['sentiment_score'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword Extraction (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=100)\n",
    "X = tfidf.fit_transform(df['cleaned_text'])\n",
    "keywords_df = pd.DataFrame(X.toarray(), columns=tfidf.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theme Clustering (Manual or Rule-Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "themes = {\n",
    "    \"Access Issues\": [\"login\", \"access\", \"reset\", \"otp\"],\n",
    "    \"Transaction Performance\": [\"slow\", \"transfer\", \"delay\", \"fail\"],\n",
    "    \"UI/UX\": [\"interface\", \"navigation\", \"easy\", \"design\"],\n",
    "    \"Customer Support\": [\"support\", \"help\", \"response\", \"contact\"]\n",
    "}\n",
    "\n",
    "def assign_theme(text):\n",
    "    assigned = []\n",
    "    for theme, keywords in themes.items():\n",
    "        if any(kw in text for kw in keywords):\n",
    "            assigned.append(theme)\n",
    "    return ', '.join(assigned)\n",
    "\n",
    "df['themes'] = df['cleaned_text'].apply(assign_theme)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Output CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('bank_reviews_with_sentiment_themes.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
